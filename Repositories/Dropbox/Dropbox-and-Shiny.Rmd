---
title: "Caching Repositories"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(csv)
library(rdrop2)
```

# Dropbox

## Bandwidth Limiting

Traffic limits for free Dropbox accounts are quite detailed here: https://www.dropbox.com/en/help/4204. At the time of writing, 20th July 2016, the limits are:

- The total amount of traffic that all of your links and file requests together can generate without getting banned is 20 GB per day.
- The total number of downloads that all of your links together can generate is 100,000 downloads per day.

# Dropbox API Access with rdrop2

The rdrop2 library provides a very useful wrapper for the Dropbox API, but we must do some work if we intend to use rdrop2 within a shiny app. This is because when initiating the `rdrop2` library we must authenticate ourselves using `drop_auth()` which expects interaction via a browser window - and we're simply not setup for that. We therefore store the authentication token against the symbol `token` and save this object using `saveRDS` for later use.

```{r, eval=FALSE, include=FALSE}
library(rdrop2)
token <- drop_auth(new_user = TRUE)
saveRDS(token, "droptoken.rds")
```

The token can then be loaded as follows:

```{r, eval=F}
token <- readRDS("droptoken.rds")
drop_acc(dtoken = token)
```

## Accessing Files

Using `drop_get(path = , local.file = )` we can download the file at `path` into the working directory with the filename `local.file`

```{r}
drop_get('/Private_Cache-Tests/UK_Prime_Ministers.csv', local_file = "pms_data.csv", dtoken = token)
library(csv)
str(read.csv("pms_data.csv"))
```

Note that if a file does not exist, `drop_get` will return `FALSE`.

```{r}
drop_get('/Private_Cache-Tests/non-existant.csv', local_file = "pms_data.csv", dtoken = token)
```

# Persistent Storage: Caching

Following advise from http://shiny.rstudio.com/articles/persistent-data-storage.html, the type of persistence we want is described as:

"you save a new file every time there is new data"

The basic procedure we should follow is:

- Check remote file exists
- FALSE: Import local file with most recent modification date
- TRUE: Next step
- Check remote file is newer than local files
- TRUE: Download remote file to local directory and import
- FALSE: Import local file with most recent modification date

There is one exception to this procedure:

- The first time the app is launched after deployment, always grab the remote file (if possible) to account for possible file changes during deployment.

Further, this procedure does not attempt to account for deltas and simply downloads a new copy of the remote file whenever it is updated - which could result in a large number of files within the shiny app. To account for this, the following step is added to the procedure:

- If more than 5 local files exist, purge the oldest 3.

The two sections below describe a script for caching on a local machine and in a shiny app.

## Local Script

Set the `original_file_name` variable and define a function for creating unique file names:

```{r}
original_file_name <- "pms_data.csv"
unique_name_fn <- function(){sprintf("%s_%s.csv", digest::digest(paste0(as.integer(Sys.time()),runif(1))), "user_downloaded")}
```

This utility function gets all local files and orders their file names according to their modification date:

```{r}
sort_locals_by_date <- function() {
  all_local_files <-
    c(original_file_name, list.files()[grepl(pattern = "user[_]downloaded", list.files())])
  all_local_files_mtime <-
    unlist(lapply(all_local_files, function(x)
      file.mtime(x)))
  sort_locals_by_date <-
    all_local_files[order(all_local_files_mtime)]
  sort_locals_by_date
}
```

This nested `if` implements our methodology above:

```{r}
## Check file exists
if (drop_exists('/Private_Cache-Tests/UK_Prime_Ministers.csv', dtoken = token)) {
  if (any(grepl(pattern = "user[_]downloaded", list.files()))) {
    ## there are updated files
    ## Get modification times for local and external file
    all_local_files <-
      c(original_file_name, list.files()[grepl(pattern = "user[_]downloaded", list.files())])
    all_local_files_mtime <-
      unlist(lapply(all_local_files, function(x)
        file.mtime(x)))
    remote_file_mtime <-
      dmy_hms(drop_history('/Private_Cache-Tests/UK_Prime_Ministers.csv', dtoken = token)[1, modified])
    
    if (!any(all_local_files_mtime > as.integer(remote_file_mtime))) {
      drop_get(
        '/Private_Cache-Tests/UK_Prime_Ministers.csv',
        local_file = store_fname,
        overwrite = T,
        dtoken = token
      )
      sorted_files <- sort_locals_by_date()
      ## Import most recently updated file
      data_to_use <-
        read.csv(sorted_files[length(sorted_files)])
    } else {
      sorted_files <- sort_locals_by_date()
      ## Import most recently updated file
      data_to_use <-
        read.csv(sorted_files[length(sorted_files)])
    }
  } else {
    ## first deploy, get file and import
    drop_get(
      '/Private_Cache-Tests/UK_Prime_Ministers.csv',
      local_file = unique_name_fn(),
      overwrite = T,
      dtoken = token
    )
    sorted_files <- sort_locals_by_date()
    ## Import most recently updated file
    data_to_use <-
      read.csv(sorted_files[length(sorted_files)])
  }
} else {
  ## if external file does not exist
  sorted_files <- sort_locals_by_date()
  ## Import most recently updated file
  data_to_use <- read.csv(sorted_files[length(sorted_files)])
}
```

The script below ensure that there are never more than 5 copies of the data in the local directory

```{r}
if(sum(grepl(pattern = "user[_]downloaded", list.files())) > 5) {
  sorted_files <- sort_locals_by_date()
  sorted_files <-
    sorted_files[grepl(pattern = "user[_]downloaded", sorted_files)]
  lapply(sorted_files[1:3], function(x)
    file.remove(x))
}
```

## Shiny Script

The directory XXXunique-filename-shinyXXX contains a shiny app that pulls from the same Dropbox file as above using the same caching methodology. The apps workds as follows:

```{r}

```





